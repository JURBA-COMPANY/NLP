import sqlite3
from nltk.tokenize import sent_tokenize, word_tokenize
from multiprocessing import cpu_count
from gensim.models.word2vec import Word2Vec

    
def vectorize():
    connect = sqlite3.connect('nlp.sqlite3')
    cursor = connect.cursor()
    data = cursor.execute('SELECT КОНТЕКСТ FROM linguist')
    megalist = []
    for i in data:
        k = list(i)
        megalist.append(k)
        for q in k:
            u = q
            q = q.replace('\xa0\xa0', ' ')
            k.append(q)
            k.remove(u)
    for i in megalist:
        for g in i:
            print(word_tokenize(g))
